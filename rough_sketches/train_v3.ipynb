{"cells":[{"cell_type":"markdown","metadata":{"id":"l1rWhsxPYGft"},"source":["<h3> Summary of notebook: </h3>\n","\n","- Now we move from word-based embeddings to BPE-based embeddings"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#! pip install tokenizers===0.9.3"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1660753571224,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Mj4NcfcoYGfw"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import string\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Bidirectional, Concatenate, LSTM, Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow.keras.initializers import Constant\n","\n","from sklearn.model_selection import train_test_split\n","\n","import re\n","import os\n","import io\n","import time"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from tokenizers import Tokenizer, ByteLevelBPETokenizer\n","from tokenizers.models import BPE\n","from tokenizers.trainers import BpeTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","from tokenizers.processors import TemplateProcessing"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3106,"status":"ok","timestamp":1660753576484,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"fMfUbKk5YGfx","outputId":"7473f42b-1584-4793-ef29-cfca39448a9b"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# %cd gdrive/MyDrive/ColabNotebooks/colab_upload\n","\n","#df_en_de = pd.read_csv('/content/gdrive/MyDrive/transformer_nmt_dataset/df_complete_30.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660753576485,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"A8zrZoxzYsS1"},"outputs":[],"source":["from model_components import preprocess_sentence, get_angles, positional_encoding, \\\n","                            create_padding_mask, create_look_ahead_mask, \\\n","                            FullyConnected, EncoderLayer, Encoder, DecoderLayer, Decoder, Transformer, CustomSchedule, \\\n","                                create_train_tokenizer, load_tokenizer"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from training_helper_functions import loss_function, accuracy_function, compute_test_metrics"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n","df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df_en_de['german'] = df_en_de['german'].apply(preprocess_sentence)\n","df_en_de['english'] = df_en_de['english'].apply(preprocess_sentence)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":728,"status":"ok","timestamp":1660753578742,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"G3QynPgBiTGR"},"outputs":[],"source":["# pre-process sentences using helper function\n","pairs = df_en_de\n","pairs = pairs.sample(frac = 0.01)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>english</th>\n","      <th>german</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>115225</th>\n","      <td>Tom has done so much for me.</td>\n","      <td>Tom hat so viel fuer mich getan.</td>\n","    </tr>\n","    <tr>\n","      <th>146067</th>\n","      <td>He told us an interesting story.</td>\n","      <td>Er erzaehlte uns eine interessante Geschichte.</td>\n","    </tr>\n","    <tr>\n","      <th>73892</th>\n","      <td>I ' m going to rent a car.</td>\n","      <td>Ich werde ein Auto mieten.</td>\n","    </tr>\n","    <tr>\n","      <th>25764</th>\n","      <td>She ' s fashionable.</td>\n","      <td>Sie ist modebewusst.</td>\n","    </tr>\n","    <tr>\n","      <th>30385</th>\n","      <td>How can I help you?</td>\n","      <td>Womit kann ich Ihnen behilflich sein?</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                 english  \\\n","115225      Tom has done so much for me.   \n","146067  He told us an interesting story.   \n","73892         I ' m going to rent a car.   \n","25764               She ' s fashionable.   \n","30385                How can I help you?   \n","\n","                                                german  \n","115225                Tom hat so viel fuer mich getan.  \n","146067  Er erzaehlte uns eine interessante Geschichte.  \n","73892                       Ich werde ein Auto mieten.  \n","25764                             Sie ist modebewusst.  \n","30385            Womit kann ich Ihnen behilflich sein?  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["pairs.head()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["2517"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(pairs)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# load pre-trained tokenizers for de and en\n","en_tokenizer, en_word_index = load_tokenizer('tokenizer_en_corpus.json')\n","de_tokenizer, de_word_index = load_tokenizer('tokenizer_de_corpus.json')"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660753582823,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"yqt2dREtYGf2","outputId":"fbe3bfac-f1c9-451e-be88-be55f2cb6ef4"},"outputs":[{"data":{"text/plain":["(29999, 29999)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["vocab_len_source = len(de_word_index.keys())\n","vocab_len_target = len(en_word_index.keys())\n","\n","vocab_len_source, vocab_len_target"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1660753584488,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"pSFERvXSYGf2"},"outputs":[],"source":["num_tokens_source = vocab_len_source + 1\n","num_tokens_target = vocab_len_target + 1"]},{"cell_type":"markdown","metadata":{},"source":["- use the loaded (pre-trained) tokenizers to tokenize the text"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["source_de = []\n","target_en = []\n","\n","# iterrate over every row of dataframe \"pairs\"\n","for idx, row in pairs.iterrows():\n","    string_de = row['german']\n","    string_en = row['english']\n","\n","    if type(string_de) == str and type(string_en) == str:\n","        # encode\n","        encoding_de = de_tokenizer.encode(string_de)\n","        encoding_en = en_tokenizer.encode(string_en)\n","        # retrieve ids (integers) and append to list\n","        source_de.append(encoding_de.ids)\n","        target_en.append(encoding_en.ids)"]},{"cell_type":"markdown","metadata":{},"source":["- run time for 100% of df_complete_30: 4m 10s"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# convert to tensors with zero-padding\n","source_tensor = tf.keras.preprocessing.sequence.pad_sequences(source_de, padding = 'post')\n","target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_en, padding = 'post')"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753584986,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"kc2sAiaSYGf3"},"outputs":[],"source":["source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n","                                                                source_tensor, target_tensor, test_size=0.2\n","                                                                )"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["'Ich weiss, dass Tom euer Freund ist.'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["de_tokenizer.decode(source_train_tensor[10])"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1312,"status":"ok","timestamp":1660753586587,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"TJRmvHIIYGf3","outputId":"d28d1478-95f4-4283-e080-c639e54a2394"},"outputs":[],"source":["# save numpy array as csv file:\n","np.savetxt('source_train_tensor.csv', source_train_tensor, delimiter = ',')\n","np.savetxt('source_test_tensor.csv', source_test_tensor, delimiter = ',')\n","np.savetxt('target_train_tensor.csv', target_train_tensor, delimiter = ',')\n","np.savetxt('target_test_tensor.csv', target_test_tensor, delimiter = ',')\n"]},{"cell_type":"markdown","metadata":{},"source":["- run time for 100% of df_complete_30: 3m"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660753586587,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"A2NUoadXYGf3"},"outputs":[],"source":["max_source_length= max(len(t) for t in source_tensor)\n","max_target_length= max(len(t) for t in  target_tensor)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753586947,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"kPhYmti0YGf3","outputId":"b12e6547-7af6-4c3f-ad17-6ee59707fb9b"},"outputs":[{"data":{"text/plain":["(43, 35)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["max_source_length, max_target_length"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":877,"status":"ok","timestamp":1660753588160,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"igd_hwb9iTGU"},"outputs":[],"source":["BATCH_SIZE = 32\n","#Create training dataset and shuffle\n","dataset_train = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n","# divide into batches\n","dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n","\n","#Create test dataset\n","dataset_test = tf.data.Dataset.from_tensor_slices((source_test_tensor, target_test_tensor)).shuffle(BATCH_SIZE)\n","dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=True)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1660753588911,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"vtr_qVkriTGV","outputId":"14da9cc9-dd30-4323-b623-edd9031c361f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 43) (32, 35)\n"]}],"source":["source_batch_train, target_batch_train =next(iter(dataset_train))\n","print(source_batch_train.shape, target_batch_train.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EhuSjWSkYGf4"},"source":["<h3> Define arguments for transformer </h3>"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1660755527254,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"mgZTVQLSYGf4"},"outputs":[],"source":["# Transformer arguments: \n","# num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","# target_vocab_size, max_positional_encoding_input,\n","# max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6\n","\n","num_layers = 4\n","embedding_dim = 64\n","num_heads = 5\n","fully_connected_dim = 128\n","input_vocab_size = num_tokens_source\n","target_vocab_size = num_tokens_target\n","max_positional_encoding_input = max_source_length\n","max_positional_encoding_target = max_target_length"]},{"cell_type":"markdown","metadata":{"id":"Ly4vh0t85N3X"},"source":["<h3> Create transformer </h3>"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1660757266179,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"S8P8Bg8MYGf4","outputId":"77b53229-f388-493a-e52f-674d13b3d53e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 43, 64)\n","(1, 35, 64)\n"]}],"source":["transformer = Transformer(\n","    num_layers=num_layers,\n","    embedding_dim=embedding_dim,\n","    num_heads=num_heads,\n","    fully_connected_dim=fully_connected_dim,\n","    input_vocab_size=input_vocab_size,\n","    target_vocab_size=target_vocab_size,\n","    max_positional_encoding_input = max_positional_encoding_input,\n","    max_positional_encoding_target = max_positional_encoding_target\n","    )"]},{"cell_type":"markdown","metadata":{"id":"uvAuEWUwYGf5"},"source":["- Create optimizer\n","- Use customised learning rate as defined in 'Attention Is All You Need' paper\n","- The learning rate increases linearly until training_step reaches \"warmup_steps\", then decays asymptotically\n","- Inputs: d_model, warmup_steps (default = 4000)"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1660757268527,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"8qYL_Vg7YGf5"},"outputs":[],"source":["learning_rate = CustomSchedule(embedding_dim, warmup_steps = 4000)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n","                                     epsilon=1e-9)"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":465,"status":"ok","timestamp":1660757270498,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Ka5uNFNEYGf5"},"outputs":[],"source":["# define loss object\n","# from_logits = False, because we apply softmax to final Dense layer of Transformer\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=False, reduction='none')\n"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1660757274023,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"objpWrBgYGf5"},"outputs":[],"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n","test_accuracy = tf.keras.metrics.Mean(name = 'test_accuracy')"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":368,"status":"ok","timestamp":1660757275756,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"guju7vbIYGf5"},"outputs":[],"source":["@tf.function\n","def train_step(inp, tar):\n","                            # inp = (m, Tx)\n","                            # tar = (m, Ty)\n","\n","\n","  tar_inp = tar[:, :-1]     # \"start_\" to last word\n","  tar_real = tar[:, 1:]     # first word to \"_end\"\n","\n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inputs = (inp, tar_inp),\n","                                 training = True)\n","    loss = loss_function(tar_real, predictions, loss_object)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  acc = accuracy_function(tar_real, predictions)\n","\n","  # store cumulative loss and acc in train_loss and train_accuracy\n","  train_loss(loss)\n","  train_accuracy(acc)"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1660757277169,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"-jxQm8iSYGf5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initialising from scratch\n"]}],"source":["checkpoint_path = './checkpoints'\n","\n","ckpt = tf.train.Checkpoint(optimizer=optimizer,\n","                                 transformer=transformer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 3)\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print('Latest checkpoint restored!')\n","else:\n","    print('Initialising from scratch')"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["print(ckpt_manager.latest_checkpoint)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["epoch_batch_list = []\n","train_loss_list = []\n","train_acc_list = []\n","test_loss_list = []\n","test_acc_list = []"]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1558553,"status":"error","timestamp":1660758836991,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"PXsDysBPYGf6","outputId":"a788c160-c44b-43be-931f-360a1b4922f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Batch 0 -- Train_Loss: 1.0076 Train_Accuracy: 0.7908\n","Epoch 1 Batch 50 -- Train_Loss: 0.9632 Train_Accuracy: 0.7867\n","Saving checkpoint after epoch 1 at ./checkpoints/ckpt-41\n","Summary -- Epoch 1 Train_Loss: 0.9756 Train_Accuracy: 0.7831     Test_Loss: 4.7419 Test_Accuracy: 0.3352\n","Time taken for 1 epoch: 20.66 secs\n","\n","Epoch 2 Batch 0 -- Train_Loss: 0.8015 Train_Accuracy: 0.8389\n","Epoch 2 Batch 50 -- Train_Loss: 0.8693 Train_Accuracy: 0.8084\n","Saving checkpoint after epoch 2 at ./checkpoints/ckpt-42\n","Summary -- Epoch 2 Train_Loss: 0.8785 Train_Accuracy: 0.8082     Test_Loss: 4.9453 Test_Accuracy: 0.3381\n","Time taken for 1 epoch: 19.76 secs\n","\n","Epoch 3 Batch 0 -- Train_Loss: 0.8001 Train_Accuracy: 0.8333\n","Epoch 3 Batch 50 -- Train_Loss: 0.7697 Train_Accuracy: 0.8332\n","Saving checkpoint after epoch 3 at ./checkpoints/ckpt-43\n","Summary -- Epoch 3 Train_Loss: 0.7787 Train_Accuracy: 0.8314     Test_Loss: 5.0634 Test_Accuracy: 0.3409\n","Time taken for 1 epoch: 20.18 secs\n","\n","Epoch 4 Batch 0 -- Train_Loss: 0.6568 Train_Accuracy: 0.8498\n","Epoch 4 Batch 50 -- Train_Loss: 0.6941 Train_Accuracy: 0.8444\n","Saving checkpoint after epoch 4 at ./checkpoints/ckpt-44\n","Summary -- Epoch 4 Train_Loss: 0.7058 Train_Accuracy: 0.8423     Test_Loss: 5.0692 Test_Accuracy: 0.3435\n","Time taken for 1 epoch: 20.16 secs\n","\n","Epoch 5 Batch 0 -- Train_Loss: 0.6062 Train_Accuracy: 0.8904\n","Epoch 5 Batch 50 -- Train_Loss: 0.6156 Train_Accuracy: 0.8651\n","Saving checkpoint after epoch 5 at ./checkpoints/ckpt-45\n","Summary -- Epoch 5 Train_Loss: 0.6288 Train_Accuracy: 0.8617     Test_Loss: 5.1574 Test_Accuracy: 0.3460\n","Time taken for 1 epoch: 20.21 secs\n","\n","Epoch 6 Batch 0 -- Train_Loss: 0.6290 Train_Accuracy: 0.8425\n","Epoch 6 Batch 50 -- Train_Loss: 0.5735 Train_Accuracy: 0.8696\n","Saving checkpoint after epoch 6 at ./checkpoints/ckpt-46\n","Summary -- Epoch 6 Train_Loss: 0.5869 Train_Accuracy: 0.8677     Test_Loss: 5.2446 Test_Accuracy: 0.3482\n","Time taken for 1 epoch: 19.97 secs\n","\n","Epoch 7 Batch 0 -- Train_Loss: 0.5360 Train_Accuracy: 0.9048\n","Epoch 7 Batch 50 -- Train_Loss: 0.5178 Train_Accuracy: 0.8825\n","Saving checkpoint after epoch 7 at ./checkpoints/ckpt-47\n","Summary -- Epoch 7 Train_Loss: 0.5297 Train_Accuracy: 0.8785     Test_Loss: 5.2272 Test_Accuracy: 0.3505\n","Time taken for 1 epoch: 20.20 secs\n","\n","Epoch 8 Batch 0 -- Train_Loss: 0.4378 Train_Accuracy: 0.9091\n","Epoch 8 Batch 50 -- Train_Loss: 0.4857 Train_Accuracy: 0.8876\n","Saving checkpoint after epoch 8 at ./checkpoints/ckpt-48\n","Summary -- Epoch 8 Train_Loss: 0.4991 Train_Accuracy: 0.8837     Test_Loss: 5.4457 Test_Accuracy: 0.3528\n","Time taken for 1 epoch: 20.34 secs\n","\n","Epoch 9 Batch 0 -- Train_Loss: 0.4276 Train_Accuracy: 0.9100\n","Epoch 9 Batch 50 -- Train_Loss: 0.4330 Train_Accuracy: 0.8966\n","Saving checkpoint after epoch 9 at ./checkpoints/ckpt-49\n","Summary -- Epoch 9 Train_Loss: 0.4429 Train_Accuracy: 0.8941     Test_Loss: 5.4773 Test_Accuracy: 0.3548\n","Time taken for 1 epoch: 19.92 secs\n","\n","Epoch 10 Batch 0 -- Train_Loss: 0.4194 Train_Accuracy: 0.8979\n","Epoch 10 Batch 50 -- Train_Loss: 0.4031 Train_Accuracy: 0.9038\n","Saving checkpoint after epoch 10 at ./checkpoints/ckpt-50\n","Summary -- Epoch 10 Train_Loss: 0.4060 Train_Accuracy: 0.9038     Test_Loss: 5.4421 Test_Accuracy: 0.3570\n","Time taken for 1 epoch: 20.27 secs\n","\n"]}],"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # reset tf Mean objects\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  # iterate over every batch (= (inp, tar) tuple) in training dataset\n","  for (batch, (inp, tar)) in enumerate(dataset_train):\n","    train_step(inp, tar)\n","\n","    if batch % 50 == 0:\n","      print(f'Epoch {epoch + 1} Batch {batch} -- Train_Loss: {train_loss.result():.4f} Train_Accuracy: {train_accuracy.result():.4f}')\n","\n","\n","    # if batch % 5000 == 0:\n","    #   ckpt_save_path = ckpt_manager.save()\n","    #   print(f'Saving checkpoint after epoch {epoch +1} batch {batch} at {ckpt_save_path}')\n","\n","    if batch % 50 == 0:\n","      epoch_batch_list.append(f'epoch_{epoch+1}_batch_{batch}')\n","      train_loss_list.append (train_loss.result().numpy())\n","      train_acc_list.append(train_accuracy.result().numpy())\n","\n","      test_loss_list.append(test_loss.result().numpy())\n","      test_acc_list.append(test_accuracy.result().numpy())\n","\n","\n","  if (epoch+1) % 1 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print(f'Saving checkpoint after epoch {epoch + 1} at {ckpt_save_path}')\n","  \n","  # after one epoch of training, compute test loss and test acc\n","  for (batch, (inp, tar)) in enumerate(dataset_test):\n","    test_loss_batch, test_accuracy_batch = compute_test_metrics(inp, tar, transformer, loss_object)\n","    # Update tf Mean objects\n","    test_loss(test_loss_batch)\n","    test_accuracy(test_accuracy_batch)\n","  \n","\n","  print(f'Summary -- Epoch {epoch + 1} Train_Loss: {train_loss.result():.4f} Train_Accuracy: {train_accuracy.result():.4f} \\\n","    Test_Loss: {test_loss.result():.4f} Test_Accuracy: {test_accuracy.result():.4f}')\n","\n","  epoch_batch_list.append(f'end of epoch {epoch+1}')\n","  train_loss_list.append (train_loss.result().numpy())\n","  train_acc_list.append(train_accuracy.result().numpy())\n","\n","  test_loss_list.append(test_loss.result().numpy())\n","  test_acc_list.append(test_accuracy.result().numpy())\n","  \n","\n","\n","  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch_batch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>test_loss</th>\n","      <th>test_acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>epoch_1_batch_0</td>\n","      <td>10.306</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>epoch_1_batch_50</td>\n","      <td>10.271</td>\n","      <td>0.013</td>\n","      <td>0.000</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>end of epoch 1</td>\n","      <td>10.259</td>\n","      <td>0.029</td>\n","      <td>10.173</td>\n","      <td>0.110</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>epoch_2_batch_0</td>\n","      <td>10.184</td>\n","      <td>0.109</td>\n","      <td>0.000</td>\n","      <td>0.110</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>epoch_2_batch_50</td>\n","      <td>10.081</td>\n","      <td>0.108</td>\n","      <td>0.000</td>\n","      <td>0.110</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>145</th>\n","      <td>epoch_9_batch_50</td>\n","      <td>0.433</td>\n","      <td>0.897</td>\n","      <td>0.000</td>\n","      <td>0.353</td>\n","    </tr>\n","    <tr>\n","      <th>146</th>\n","      <td>end of epoch 9</td>\n","      <td>0.443</td>\n","      <td>0.894</td>\n","      <td>5.477</td>\n","      <td>0.355</td>\n","    </tr>\n","    <tr>\n","      <th>147</th>\n","      <td>epoch_10_batch_0</td>\n","      <td>0.419</td>\n","      <td>0.898</td>\n","      <td>0.000</td>\n","      <td>0.355</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>epoch_10_batch_50</td>\n","      <td>0.403</td>\n","      <td>0.904</td>\n","      <td>0.000</td>\n","      <td>0.355</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>end of epoch 10</td>\n","      <td>0.406</td>\n","      <td>0.904</td>\n","      <td>5.442</td>\n","      <td>0.357</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>150 rows × 5 columns</p>\n","</div>"],"text/plain":["           epoch_batch  train_loss  train_acc  test_loss  test_acc\n","0      epoch_1_batch_0      10.306      0.000      0.000     0.000\n","1     epoch_1_batch_50      10.271      0.013      0.000     0.000\n","2       end of epoch 1      10.259      0.029     10.173     0.110\n","3      epoch_2_batch_0      10.184      0.109      0.000     0.110\n","4     epoch_2_batch_50      10.081      0.108      0.000     0.110\n","..                 ...         ...        ...        ...       ...\n","145   epoch_9_batch_50       0.433      0.897      0.000     0.353\n","146     end of epoch 9       0.443      0.894      5.477     0.355\n","147   epoch_10_batch_0       0.419      0.898      0.000     0.355\n","148  epoch_10_batch_50       0.403      0.904      0.000     0.355\n","149    end of epoch 10       0.406      0.904      5.442     0.357\n","\n","[150 rows x 5 columns]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["all_metrics = zip(epoch_batch_list, train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n","df_metrics = pd.DataFrame(all_metrics, columns = ['epoch_batch', 'train_loss', 'train_acc', 'test_loss', 'test_acc'])\n","#df_metrics['epoch'] = df_metrics['epoch'].apply(lambda x: x+1)\n","#df_metrics = df_metrics.apply(lambda x: round(x, 3))\n","df_metrics[['train_loss', 'train_acc', 'test_loss', 'test_acc']] = df_metrics[['train_loss', 'train_acc', 'test_loss', 'test_acc']].apply(lambda x: round(x,3))\n","df_metrics"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660748525271,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"9va6cEFqYGf6"},"outputs":[],"source":["df_metrics.to_csv('df_metrics.csv', index = False)"]},{"cell_type":"code","execution_count":59,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660748525271,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"dTw-6gRR3m1v"},"outputs":[],"source":["# with open(\"params.txt\", \"a\") as text_file:\n","#     text_file.write(params_3)\n","#     #text_file.write('params_3 -- time taken for 1 epoch: 54 secs')"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":2283,"status":"ok","timestamp":1660748527550,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"C7NiFdQSYGf6"},"outputs":[],"source":["file_path = 'saved_models/model'\n","transformer.save_weights(file_path,save_format='tf')\n","\n","# # Recreate the exact same model purely from the file\n","# new_model = keras.models.load_model('path_to_my_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvmMhr8BOUKd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train_v1.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.4 ('tf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a30d934768a106a0bdaa0b54b1b0ce58ac936216b32c4f047caaf50b54e34c32"}}},"nbformat":4,"nbformat_minor":0}
