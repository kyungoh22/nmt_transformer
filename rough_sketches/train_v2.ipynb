{"cells":[{"cell_type":"markdown","metadata":{"id":"l1rWhsxPYGft"},"source":["<h3> Summary of notebook: </h3>\n","\n","- The notebook \"translator_transformer_tidy_mode_v1.ipynb\" has all the pieces.\n","- Here I separate the code into a script for training and a script for translating\n","- Use this script (\"train_v1.ipynb\") to train the model; save the source arrays, target arrays, dictionaries, model weights\n","- Use the next script (\"translate_v1.ipynb\") to load the model and predict\n","- *** In \"translate_v1.ipynb\", you need to instantiate the Transformer with the same parameters from the \"train_v1.ipynb\" script ***\n","- Then you load the model weights and use the functions to predict new sentences + plot attention weights"]},{"cell_type":"markdown","metadata":{},"source":["- In google colab, I experimented with some sets of (hyper) parameters, using 10% of the data (see the file \"Training_Observations.docx\")\n","- So far, the most promising are: embedding_dim = 256, num_heads = 8, num_layers = 4, fully_connected_dim = 512\n","- Increasing any of the parameters meant the accuracy reached a peak and then decreased to a useless value. \n","- You could investigate further why this happened; with \"Params 3\", I also tried training with 50% of the data, but the accuracy still showed the strange behaviour."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1660753571224,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Mj4NcfcoYGfw"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import string\n","from string import digits\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import tensorflow as tf\n","from tensorflow.keras.layers import Bidirectional, Concatenate, LSTM, Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow.keras.initializers import Constant\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import re\n","import os\n","import io\n","import time"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3106,"status":"ok","timestamp":1660753576484,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"fMfUbKk5YGfx","outputId":"7473f42b-1584-4793-ef29-cfca39448a9b"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# %cd gdrive/MyDrive/ColabNotebooks/train_translate\n","\n","# df_en_de = pd.read_table('/content/gdrive/MyDrive/ColabNotebooks/train_translate/df_complete.csv')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660753576485,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"A8zrZoxzYsS1"},"outputs":[],"source":["from model_components import preprocess_sentence, get_angles, positional_encoding, create_padding_mask, create_look_ahead_mask, FullyConnected, EncoderLayer, Encoder, DecoderLayer, Decoder, Transformer, CustomSchedule"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753577657,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"SSF9N_JhYGfy"},"outputs":[],"source":["df_en_de = pd.read_csv('df_complete.csv')"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":728,"status":"ok","timestamp":1660753578742,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"G3QynPgBiTGR"},"outputs":[],"source":["# pre-process sentences using helper function\n","\n","pairs = df_en_de\n","pairs = pairs.sample(frac = 0.001)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["2172"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(pairs)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753578742,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"1AA7rM5ziTGR"},"outputs":[],"source":["# define source and target\n","\n","source = pairs['german']\n","target = pairs ['english']"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1450,"status":"ok","timestamp":1660753580687,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"t5MIVnn2iTGS"},"outputs":[],"source":["# create tokenizer & tensor for source and target\n","source_sentence_tokenizer= Tokenizer(filters='')\n","source_sentence_tokenizer.fit_on_texts(source)\n","source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n","source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post' )\n","\n","target_sentence_tokenizer= Tokenizer(filters='')\n","target_sentence_tokenizer.fit_on_texts(target)\n","target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n","target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post' )"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1660753580976,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"U5yTQ-vttqJA"},"outputs":[],"source":["# Create word to index and index to word mappings for source and target\n","\n","source_word_index = source_sentence_tokenizer.word_index\n","target_word_index = target_sentence_tokenizer.word_index\n","\n","source_index_word = source_sentence_tokenizer.index_word\n","target_index_word = target_sentence_tokenizer.index_word"]},{"cell_type":"markdown","metadata":{"id":"3o14U-GLYGf0"},"source":["<h3> Convert Dictionary into DataFrame + Convert DataFrame into Dictionary </h3>"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753581766,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"P33xIBoKYGf1"},"outputs":[],"source":["# Convert dictionary into dataframe\n","# This will be used in the training environment. \n","# Once you fit the tokenizer to the training set and create a word_index dictionary,\n","# you save the dictionary as a csv file. \n","def df_word_index(dictionary):\n","    df = pd.DataFrame.from_dict(dictionary, orient = 'index', columns= ['index']).reset_index()\n","    df = df.rename(columns = {'level_0':'word'})\n","    return df\n","\n","def df_to_dict (df):\n","    dict_word_index = {row['word']:row['index'] for index, row in df.iterrows()}\n","    dict_index_word = {row['index']: row['word'] for index, row in df.iterrows()}\n","    return dict_word_index, dict_index_word"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660753582170,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"5q80UnPvYGf1","outputId":"b544c25d-145b-4258-e769-d6e2974be736"},"outputs":[],"source":["# convert dictionary into dataframes\n","df_source_word_index = df_word_index(source_word_index)\n","df_target_word_index = df_word_index(target_word_index)\n","\n","# save dataframes as csv files; they will be loaded in the \"translate_v1.ipynb\" script\n","df_source_word_index.to_csv('df_source_word_index.csv', index = False)\n","df_target_word_index.to_csv('df_target_word_index.csv', index = False)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660753582823,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"yqt2dREtYGf2","outputId":"fbe3bfac-f1c9-451e-be88-be55f2cb6ef4"},"outputs":[{"data":{"text/plain":["(8715, 6064)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["vocab_len_source = len(source_word_index.keys())\n","vocab_len_target = len(target_word_index.keys())\n","vocab_len_source, vocab_len_target"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1660753584488,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"pSFERvXSYGf2"},"outputs":[],"source":["num_tokens_source = vocab_len_source + 1\n","num_tokens_target = vocab_len_target + 1"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753584986,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"kc2sAiaSYGf3"},"outputs":[],"source":["source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n","                                                                source_tensor, target_tensor,test_size=0.2\n","                                                                )"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1312,"status":"ok","timestamp":1660753586587,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"TJRmvHIIYGf3","outputId":"d28d1478-95f4-4283-e080-c639e54a2394"},"outputs":[{"data":{"text/plain":["'\\nFor translating mode\\n'"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# save numpy array as csv file:\n","np.savetxt('source_train_tensor.csv', source_train_tensor, delimiter = ',')\n","np.savetxt('source_test_tensor.csv', source_test_tensor, delimiter = ',')\n","np.savetxt('target_train_tensor.csv', target_train_tensor, delimiter = ',')\n","np.savetxt('target_test_tensor.csv', target_test_tensor, delimiter = ',')\n","\n","\n","\"\"\"\n","For translating mode\n","\"\"\"\n","# # load numpy array from csv file:\n","# load_ar = np.loadtxt('source_train_tensor.csv', delimiter = ',')"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660753586587,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"A2NUoadXYGf3"},"outputs":[],"source":["max_source_length= max(len(t) for t in source_tensor)\n","max_target_length= max(len(t) for t in  target_tensor)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753586947,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"kPhYmti0YGf3","outputId":"b12e6547-7af6-4c3f-ad17-6ee59707fb9b"},"outputs":[{"data":{"text/plain":["(107, 110)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["max_source_length, max_target_length"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":877,"status":"ok","timestamp":1660753588160,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"igd_hwb9iTGU"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-08-18 12:24:26.293468: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["BATCH_SIZE = 32\n","#Create training dataset and shuffle\n","dataset_train = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n","# divide into batches\n","dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n","\n","#Create test dataset\n","dataset_test = tf.data.Dataset.from_tensor_slices((source_test_tensor, target_test_tensor)).shuffle(BATCH_SIZE)\n","dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=True)\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1660753588911,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"vtr_qVkriTGV","outputId":"14da9cc9-dd30-4323-b623-edd9031c361f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 107) (32, 110)\n"]}],"source":["source_batch_train, target_batch_train =next(iter(dataset_train))\n","print(source_batch_train.shape, target_batch_train.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EhuSjWSkYGf4"},"source":["<h3> Parameters 6 </h3>"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1660755527254,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"mgZTVQLSYGf4"},"outputs":[],"source":["# Transformer arguments: \n","# num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","# target_vocab_size, max_positional_encoding_input,\n","# max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6\n","\n","num_layers = 4\n","embedding_dim = 64\n","num_heads = 5\n","fully_connected_dim = 512\n","input_vocab_size = num_tokens_source\n","target_vocab_size = num_tokens_target\n","max_positional_encoding_input = max_source_length\n","max_positional_encoding_target = max_target_length"]},{"cell_type":"markdown","metadata":{"id":"Ly4vh0t85N3X"},"source":["<h3> Modify parameter number </h3>"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1660755530184,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Z4gCP4450_RN"},"outputs":[],"source":["# params_3 = f'\\n params_3: num_layers: {num_layers}, embedding_dim: {embedding_dim}, num_heads: {num_heads}, \\\n","# fully_connected_dim: {fully_connected_dim}, input_vocab_size: {input_vocab_size}, target_vocab_size: {target_vocab_size}, \\\n","# max_positional_encoding_input: {max_positional_encoding_input}, max_positional_encoding_target: {max_positional_encoding_target}'"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1660757266179,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"S8P8Bg8MYGf4","outputId":"77b53229-f388-493a-e52f-674d13b3d53e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 107, 64)\n","(1, 110, 64)\n"]}],"source":["# Transformer arguments: \n","# num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","# target_vocab_size, max_positional_encoding_input,\n","# max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6\n","\n","transformer = Transformer(\n","    num_layers=num_layers,\n","    embedding_dim=embedding_dim,\n","    num_heads=num_heads,\n","    fully_connected_dim=fully_connected_dim,\n","    input_vocab_size=input_vocab_size,\n","    target_vocab_size=target_vocab_size,\n","    max_positional_encoding_input = max_positional_encoding_input,\n","    max_positional_encoding_target = max_positional_encoding_target\n","    )"]},{"cell_type":"markdown","metadata":{"id":"uvAuEWUwYGf5"},"source":["- Create optimizer\n","- Use customised learning rate as defined in 'Attention Is All You Need' paper\n","- The learning rate increases linearly until \"warmup_steps\" training steps, then decays asymptotically\n","- Inputs: d_model, warmup_steps (default = 4000)"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1660757268527,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"8qYL_Vg7YGf5"},"outputs":[],"source":["learning_rate = CustomSchedule(embedding_dim, warmup_steps = 2500)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n","                                     epsilon=1e-9)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":465,"status":"ok","timestamp":1660757270498,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Ka5uNFNEYGf5"},"outputs":[],"source":["# define loss object\n","# from_logits = False, because we apply softmax to final Dense layer of Transformer\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=False, reduction='none')\n"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":304,"status":"ok","timestamp":1660757272651,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"P4p4NUnqYGf5"},"outputs":[],"source":["def loss_function(real, pred):\n","                                                            # real = (m, Ty)\n","                                                            # pred = (m, Ty, num_tokens_target)\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))        # want to select only non-zero values\n","                                                            # mask = (m, Ty), and is \"True\" for non-zero values\n","\n","  loss_ = loss_object(real, pred)                           # compute loss for each time-step\n","                                                            # loss = (m, Ty)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)                   \n","  loss_ *= mask                                             # only count loss from non-zero values\n","\n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)           # divide sum(loss) by number of non-zero values\n","\n","def accuracy_function(real, pred):                          # pred = (m, Ty, num_tokens_target)\n","  \n","  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), tf.int32))      # accuracies = (m, Ty) -- binary values\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))        # mask = (m, Ty) -- boolean values\n","  accuracies = tf.math.logical_and(mask, accuracies)        # suppress values where real value is 0\n","\n","  accuracies = tf.cast(accuracies, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)      # divide sum of 1s in \"accuracies\" by sum of 1s in \"mask\""]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1660757274023,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"objpWrBgYGf5"},"outputs":[],"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n","test_accuracy = tf.keras.metrics.Mean(name = 'test_accuracy')"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":368,"status":"ok","timestamp":1660757275756,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"guju7vbIYGf5"},"outputs":[],"source":["@tf.function\n","def train_step(inp, tar):\n","                            # inp = (m, Tx)\n","                            # tar = (m, Ty)\n","\n","\n","  tar_inp = tar[:, :-1]     # \"start_\" to last word\n","  tar_real = tar[:, 1:]     # first word to \"_end\"\n","\n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inputs = (inp, tar_inp),\n","                                 training = True)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  acc = accuracy_function(tar_real, predictions)\n","\n","  # store cumulative loss and acc in train_loss and train_accuracy\n","  train_loss(loss)\n","  train_accuracy(acc)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def compute_test_loss_acc(inp, tar):\n","    # inp = (m, Tx)\n","    # tar = (m, Ty)\n","\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    predictions, _ = transformer (inputs = (inp, tar_inp), training = False)\n","    test_loss = loss_function(tar_real, predictions)\n","    test_acc = accuracy_function(tar_real, predictions)\n","\n","    return test_loss, test_acc"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1660757277169,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"-jxQm8iSYGf5"},"outputs":[],"source":["# checkpoint_path = './checkpoints/train'\n","\n","# ckpt = tf.train.Checkpoint(optimizer=optimizer,\n","#                                  transformer=transformer)\n","\n","# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 3)\n","# if ckpt_manager.latest_checkpoint:\n","#     ckpt.restore(ckpt_manager.latest_checkpoint)\n","#     print('Latest checkpoint restored!')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loss_list = []\n","train_acc_list = []\n","test_loss_list = []\n","test_acc_list = []"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1558553,"status":"error","timestamp":1660758836991,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"PXsDysBPYGf6","outputId":"a788c160-c44b-43be-931f-360a1b4922f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Batch 0 -- Train_Loss: 4.1068 Train_Accuracy: 0.2891\n","Epoch 1 Batch 50 -- Train_Loss: 4.0722 Train_Accuracy: 0.3123\n","Summary -- Epoch 1 Train_Loss: 4.0722 Train_Accuracy: 0.3131     Test_Loss: 6.0572 Test_Accuracy: 0.1463\n","Time taken for 1 epoch: 59.12 secs\n","\n","Epoch 2 Batch 0 -- Train_Loss: 4.0118 Train_Accuracy: 0.3277\n","Epoch 2 Batch 50 -- Train_Loss: 3.9154 Train_Accuracy: 0.3280\n","Summary -- Epoch 2 Train_Loss: 3.9141 Train_Accuracy: 0.3284     Test_Loss: 6.1273 Test_Accuracy: 0.1488\n","Time taken for 1 epoch: 58.49 secs\n","\n","Epoch 3 Batch 0 -- Train_Loss: 3.8215 Train_Accuracy: 0.3148\n","Epoch 3 Batch 50 -- Train_Loss: 3.7642 Train_Accuracy: 0.3408\n","Summary -- Epoch 3 Train_Loss: 3.7646 Train_Accuracy: 0.3413     Test_Loss: 6.2507 Test_Accuracy: 0.1508\n","Time taken for 1 epoch: 58.51 secs\n","\n","Epoch 4 Batch 0 -- Train_Loss: 3.7601 Train_Accuracy: 0.3338\n","Epoch 4 Batch 50 -- Train_Loss: 3.6303 Train_Accuracy: 0.3512\n","Summary -- Epoch 4 Train_Loss: 3.6307 Train_Accuracy: 0.3515     Test_Loss: 6.2399 Test_Accuracy: 0.1527\n","Time taken for 1 epoch: 58.87 secs\n","\n","Epoch 5 Batch 0 -- Train_Loss: 3.6053 Train_Accuracy: 0.3571\n","Epoch 5 Batch 50 -- Train_Loss: 3.4647 Train_Accuracy: 0.3706\n","Summary -- Epoch 5 Train_Loss: 3.4616 Train_Accuracy: 0.3715     Test_Loss: 6.2673 Test_Accuracy: 0.1545\n","Time taken for 1 epoch: 60.76 secs\n","\n","Epoch 6 Batch 0 -- Train_Loss: 3.4683 Train_Accuracy: 0.3682\n","Epoch 6 Batch 50 -- Train_Loss: 3.3217 Train_Accuracy: 0.3828\n","Summary -- Epoch 6 Train_Loss: 3.3184 Train_Accuracy: 0.3836     Test_Loss: 6.4021 Test_Accuracy: 0.1563\n","Time taken for 1 epoch: 59.90 secs\n","\n","Epoch 7 Batch 0 -- Train_Loss: 3.2005 Train_Accuracy: 0.4028\n","Epoch 7 Batch 50 -- Train_Loss: 3.1679 Train_Accuracy: 0.4029\n","Summary -- Epoch 7 Train_Loss: 3.1696 Train_Accuracy: 0.4025     Test_Loss: 6.4288 Test_Accuracy: 0.1576\n","Time taken for 1 epoch: 59.77 secs\n","\n","Epoch 8 Batch 0 -- Train_Loss: 3.0760 Train_Accuracy: 0.4196\n","Epoch 8 Batch 50 -- Train_Loss: 3.0074 Train_Accuracy: 0.4196\n","Summary -- Epoch 8 Train_Loss: 3.0058 Train_Accuracy: 0.4204     Test_Loss: 6.5341 Test_Accuracy: 0.1589\n","Time taken for 1 epoch: 58.62 secs\n","\n","Epoch 9 Batch 0 -- Train_Loss: 2.9925 Train_Accuracy: 0.4140\n","Epoch 9 Batch 50 -- Train_Loss: 2.8395 Train_Accuracy: 0.4422\n","Summary -- Epoch 9 Train_Loss: 2.8391 Train_Accuracy: 0.4425     Test_Loss: 6.6661 Test_Accuracy: 0.1595\n","Time taken for 1 epoch: 58.60 secs\n","\n","Epoch 10 Batch 0 -- Train_Loss: 2.8272 Train_Accuracy: 0.4547\n","Epoch 10 Batch 50 -- Train_Loss: 2.6787 Train_Accuracy: 0.4662\n","Summary -- Epoch 10 Train_Loss: 2.6756 Train_Accuracy: 0.4668     Test_Loss: 6.7758 Test_Accuracy: 0.1606\n","Time taken for 1 epoch: 58.82 secs\n","\n"]}],"source":["EPOCHS = 10\n","# train_loss_dict = {}\n","# train_acc_dict = {}\n","# test_loss_dict = {}\n","# test_acc_dict = {}\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # reset tf Mean objects\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  # iterate over every batch (= (inp, tar) tuple) in training dataset\n","  for (batch, (inp, tar)) in enumerate(dataset_train):\n","    train_step(inp, tar)\n","\n","    if batch % 50 == 0:\n","      print(f'Epoch {epoch + 1} Batch {batch} -- Train_Loss: {train_loss.result():.4f} Train_Accuracy: {train_accuracy.result():.4f}')\n","\n","  #if (epoch+1) % 1 == 0:\n","    #ckpt_save_path = ckpt_manager.save()\n","    #print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n","  \n","  # after one epoch of training, compute test loss and test acc\n","  for (batch, (inp, tar)) in enumerate(dataset_test):\n","    test_loss_batch, test_accuracy_batch = compute_test_loss_acc(inp, tar)\n","    # Update tf Mean objects\n","    test_loss(test_loss_batch)\n","    test_accuracy(test_accuracy_batch)\n","  \n","\n","\n","  print(f'Summary -- Epoch {epoch + 1} Train_Loss: {train_loss.result():.4f} Train_Accuracy: {train_accuracy.result():.4f} \\\n","    Test_Loss: {test_loss.result():.4f} Test_Accuracy: {test_accuracy.result():.4f}')\n","  \n","  train_loss_list.append (train_loss.result().numpy())\n","  train_acc_list.append(train_accuracy.result().numpy())\n","\n","  test_loss_list.append(test_loss.result().numpy())\n","  test_acc_list.append(test_accuracy.result().numpy())\n","\n","  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>test_loss</th>\n","      <th>test_acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>8.662</td>\n","      <td>0.006</td>\n","      <td>8.556</td>\n","      <td>0.051</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>8.441</td>\n","      <td>0.053</td>\n","      <td>8.304</td>\n","      <td>0.062</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>8.103</td>\n","      <td>0.071</td>\n","      <td>7.915</td>\n","      <td>0.068</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>7.635</td>\n","      <td>0.082</td>\n","      <td>7.430</td>\n","      <td>0.071</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>7.109</td>\n","      <td>0.083</td>\n","      <td>6.955</td>\n","      <td>0.072</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>6.650</td>\n","      <td>0.089</td>\n","      <td>6.617</td>\n","      <td>0.077</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>6.374</td>\n","      <td>0.100</td>\n","      <td>6.444</td>\n","      <td>0.081</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>6.200</td>\n","      <td>0.115</td>\n","      <td>6.309</td>\n","      <td>0.086</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>6.004</td>\n","      <td>0.138</td>\n","      <td>6.173</td>\n","      <td>0.092</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>5.791</td>\n","      <td>0.159</td>\n","      <td>6.071</td>\n","      <td>0.098</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>5.582</td>\n","      <td>0.184</td>\n","      <td>6.011</td>\n","      <td>0.104</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>5.388</td>\n","      <td>0.205</td>\n","      <td>5.966</td>\n","      <td>0.109</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>5.220</td>\n","      <td>0.219</td>\n","      <td>5.920</td>\n","      <td>0.115</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>5.056</td>\n","      <td>0.234</td>\n","      <td>5.921</td>\n","      <td>0.120</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>4.897</td>\n","      <td>0.248</td>\n","      <td>5.931</td>\n","      <td>0.125</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>4.755</td>\n","      <td>0.259</td>\n","      <td>5.934</td>\n","      <td>0.130</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>4.602</td>\n","      <td>0.270</td>\n","      <td>5.983</td>\n","      <td>0.133</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>4.474</td>\n","      <td>0.281</td>\n","      <td>5.944</td>\n","      <td>0.137</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>4.339</td>\n","      <td>0.292</td>\n","      <td>5.967</td>\n","      <td>0.141</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>4.218</td>\n","      <td>0.302</td>\n","      <td>5.942</td>\n","      <td>0.144</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>21</td>\n","      <td>4.072</td>\n","      <td>0.313</td>\n","      <td>6.057</td>\n","      <td>0.146</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>22</td>\n","      <td>3.914</td>\n","      <td>0.328</td>\n","      <td>6.127</td>\n","      <td>0.149</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>23</td>\n","      <td>3.765</td>\n","      <td>0.341</td>\n","      <td>6.251</td>\n","      <td>0.151</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>24</td>\n","      <td>3.631</td>\n","      <td>0.351</td>\n","      <td>6.240</td>\n","      <td>0.153</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>25</td>\n","      <td>3.462</td>\n","      <td>0.371</td>\n","      <td>6.267</td>\n","      <td>0.154</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>26</td>\n","      <td>3.318</td>\n","      <td>0.384</td>\n","      <td>6.402</td>\n","      <td>0.156</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>27</td>\n","      <td>3.170</td>\n","      <td>0.402</td>\n","      <td>6.429</td>\n","      <td>0.158</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>28</td>\n","      <td>3.006</td>\n","      <td>0.420</td>\n","      <td>6.534</td>\n","      <td>0.159</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>29</td>\n","      <td>2.839</td>\n","      <td>0.443</td>\n","      <td>6.666</td>\n","      <td>0.160</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>30</td>\n","      <td>2.676</td>\n","      <td>0.467</td>\n","      <td>6.776</td>\n","      <td>0.161</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    epoch  train_loss  train_acc  test_loss  test_acc\n","0       1       8.662      0.006      8.556     0.051\n","1       2       8.441      0.053      8.304     0.062\n","2       3       8.103      0.071      7.915     0.068\n","3       4       7.635      0.082      7.430     0.071\n","4       5       7.109      0.083      6.955     0.072\n","5       6       6.650      0.089      6.617     0.077\n","6       7       6.374      0.100      6.444     0.081\n","7       8       6.200      0.115      6.309     0.086\n","8       9       6.004      0.138      6.173     0.092\n","9      10       5.791      0.159      6.071     0.098\n","10     11       5.582      0.184      6.011     0.104\n","11     12       5.388      0.205      5.966     0.109\n","12     13       5.220      0.219      5.920     0.115\n","13     14       5.056      0.234      5.921     0.120\n","14     15       4.897      0.248      5.931     0.125\n","15     16       4.755      0.259      5.934     0.130\n","16     17       4.602      0.270      5.983     0.133\n","17     18       4.474      0.281      5.944     0.137\n","18     19       4.339      0.292      5.967     0.141\n","19     20       4.218      0.302      5.942     0.144\n","20     21       4.072      0.313      6.057     0.146\n","21     22       3.914      0.328      6.127     0.149\n","22     23       3.765      0.341      6.251     0.151\n","23     24       3.631      0.351      6.240     0.153\n","24     25       3.462      0.371      6.267     0.154\n","25     26       3.318      0.384      6.402     0.156\n","26     27       3.170      0.402      6.429     0.158\n","27     28       3.006      0.420      6.534     0.159\n","28     29       2.839      0.443      6.666     0.160\n","29     30       2.676      0.467      6.776     0.161"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["all_metrics = zip(train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n","df_metrics = pd.DataFrame(all_metrics, columns = ['train_loss', 'train_acc', 'test_loss', 'test_acc']).reset_index().rename(columns = {'index':'epoch'})\n","df_metrics['epoch'] = df_metrics['epoch'].apply(lambda x: x+1)\n","df_metrics = df_metrics.apply(lambda x: round(x, 3))\n","df_metrics"]},{"cell_type":"code","execution_count":37,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660748525271,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"9va6cEFqYGf6"},"outputs":[],"source":["df_metrics.to_csv('df_metrics.csv', index = False)"]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660748525271,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"dTw-6gRR3m1v"},"outputs":[],"source":["# with open(\"params.txt\", \"a\") as text_file:\n","#     text_file.write(params_3)\n","#     #text_file.write('params_3 -- time taken for 1 epoch: 54 secs')"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":2283,"status":"ok","timestamp":1660748527550,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"C7NiFdQSYGf6"},"outputs":[],"source":["file_path = 'saved_models/model'\n","transformer.save_weights(file_path,save_format='tf')\n","\n","# # Recreate the exact same model purely from the file\n","# new_model = keras.models.load_model('path_to_my_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvmMhr8BOUKd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train_v1.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.4 ('tf')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"a30d934768a106a0bdaa0b54b1b0ce58ac936216b32c4f047caaf50b54e34c32"}}},"nbformat":4,"nbformat_minor":0}
