{"cells":[{"cell_type":"markdown","metadata":{"id":"l1rWhsxPYGft"},"source":["<h3> Summary of notebook: </h3>\n","\n","- The notebook \"translator_transformer_tidy_mode_v1.ipynb\" has all the pieces.\n","- Here I separate the code into a script for training and a script for translating\n","- Use this script (\"train_v1.ipynb\") to train the model; save the source arrays, target arrays, dictionaries, model weights\n","- Use the next script (\"translate_v1.ipynb\") to load the model and predict\n","- *** In \"translate_v1.ipynb\", you need to instantiate the Transformer with the same parameters from the \"train_v1.ipynb\" script ***\n","- Then you load the model weights and use the functions to predict new sentences + plot attention weights"]},{"cell_type":"markdown","metadata":{},"source":["- In google colab, I experimented with some sets of (hyper) parameters, using 10% of the data (see the file \"Training_Observations.docx\")\n","- So far, the most promising are: embedding_dim = 256, num_heads = 8, num_layers = 4, fully_connected_dim = 512\n","- Increasing any of the parameters meant the accuracy reached a peak and then decreased to a useless value. \n","- You could investigate further why this happened; with \"Params 3\", I also tried training with 50% of the data, but the accuracy still showed the strange behaviour."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1660753571224,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Mj4NcfcoYGfw"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import string\n","from string import digits\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import tensorflow as tf\n","from tensorflow.keras.layers import Bidirectional, Concatenate, LSTM, Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n","from tensorflow.keras.initializers import Constant\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import re\n","import os\n","import io\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3106,"status":"ok","timestamp":1660753576484,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"fMfUbKk5YGfx","outputId":"7473f42b-1584-4793-ef29-cfca39448a9b"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","# %cd gdrive/MyDrive/ColabNotebooks/train_translate\n","\n","# df_en_de = pd.read_table('/content/gdrive/MyDrive/deu-eng/deu.txt', names=['eng', 'deu', 'attr'])"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660753576485,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"A8zrZoxzYsS1"},"outputs":[],"source":["from model_components import preprocess_sentence, get_angles, positional_encoding, create_padding_mask, create_look_ahead_mask, FullyConnected, EncoderLayer, Encoder, DecoderLayer, Decoder, Transformer, CustomSchedule"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753577657,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"SSF9N_JhYGfy"},"outputs":[],"source":["df_en_de = pd.read_table('deu-eng/deu.txt', names=['eng', 'deu', 'attr'])\n","df_en_de = df_en_de.drop('attr',axis = 1).rename(columns = {'eng':'english', 'deu':'german'})"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":728,"status":"ok","timestamp":1660753578742,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"G3QynPgBiTGR"},"outputs":[],"source":["# pre-process sentences using helper function\n","\n","pairs = df_en_de\n","pairs = pairs.sample(frac = 0.1)\n","pairs['english'] = pairs['english'].apply(preprocess_sentence)\n","pairs['german'] = pairs['german'].apply(preprocess_sentence)"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[{"data":{"text/plain":["252"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["len(pairs)"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753578742,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"1AA7rM5ziTGR"},"outputs":[],"source":["# define source and target\n","\n","source = pairs['german']\n","target = pairs ['english']"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":1450,"status":"ok","timestamp":1660753580687,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"t5MIVnn2iTGS"},"outputs":[],"source":["# create tokenizer & tensor for source and target\n","source_sentence_tokenizer= Tokenizer(filters='')\n","source_sentence_tokenizer.fit_on_texts(source)\n","source_tensor = source_sentence_tokenizer.texts_to_sequences(source)\n","source_tensor= tf.keras.preprocessing.sequence.pad_sequences(source_tensor, padding='post' )\n","\n","target_sentence_tokenizer= Tokenizer(filters='')\n","target_sentence_tokenizer.fit_on_texts(target)\n","target_tensor = target_sentence_tokenizer.texts_to_sequences(target)\n","target_tensor= tf.keras.preprocessing.sequence.pad_sequences(target_tensor, padding='post' )"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1660753580976,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"U5yTQ-vttqJA"},"outputs":[],"source":["# Create word to index and index to word mappings for source and target\n","\n","source_word_index = source_sentence_tokenizer.word_index\n","target_word_index = target_sentence_tokenizer.word_index\n","\n","source_index_word = source_sentence_tokenizer.index_word\n","target_index_word = target_sentence_tokenizer.index_word"]},{"cell_type":"markdown","metadata":{"id":"3o14U-GLYGf0"},"source":["<h3> Convert Dictionary into DataFrame + Convert DataFrame into Dictionary </h3>"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753581766,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"P33xIBoKYGf1"},"outputs":[],"source":["# Convert dictionary into dataframe\n","# This will be used in the training environment. \n","# Once you fit the tokenizer to the training set and create a word_index dictionary,\n","# you save the dictionary as a csv file. \n","def df_word_index(dictionary):\n","    df = pd.DataFrame.from_dict(dictionary, orient = 'index', columns= ['index']).reset_index()\n","    df = df.rename(columns = {'level_0':'word'})\n","    return df\n","\n","def df_to_dict (df):\n","    dict_word_index = {row['word']:row['index'] for index, row in df.iterrows()}\n","    dict_index_word = {row['index']: row['word'] for index, row in df.iterrows()}\n","    return dict_word_index, dict_index_word"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660753582170,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"5q80UnPvYGf1","outputId":"b544c25d-145b-4258-e769-d6e2974be736"},"outputs":[],"source":["# convert dictionary into dataframes\n","df_source_word_index = df_word_index(source_word_index)\n","df_target_word_index = df_word_index(target_word_index)\n","\n","# save dataframes as csv files; they will be loaded in the \"translate_v1.ipynb\" script\n","df_source_word_index.to_csv('df_source_word_index.csv', index = False)\n","df_target_word_index.to_csv('df_target_word_index.csv', index = False)"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660753582823,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"yqt2dREtYGf2","outputId":"fbe3bfac-f1c9-451e-be88-be55f2cb6ef4"},"outputs":[{"data":{"text/plain":["(682, 619)"]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["vocab_len_source = len(source_word_index.keys())\n","vocab_len_target = len(target_word_index.keys())\n","vocab_len_source, vocab_len_target"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1660753584488,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"pSFERvXSYGf2"},"outputs":[],"source":["num_tokens_source = vocab_len_source + 1\n","num_tokens_target = vocab_len_target + 1"]},{"cell_type":"code","execution_count":97,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753584986,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"kc2sAiaSYGf3"},"outputs":[],"source":["source_train_tensor, source_test_tensor, target_train_tensor, target_test_tensor = train_test_split(\n","                                                                source_tensor, target_tensor,test_size=0.2\n","                                                                )"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1312,"status":"ok","timestamp":1660753586587,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"TJRmvHIIYGf3","outputId":"d28d1478-95f4-4283-e080-c639e54a2394"},"outputs":[{"data":{"text/plain":["'\\nFor translating mode\\n'"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["# save numpy array as csv file:\n","np.savetxt('source_train_tensor.csv', source_train_tensor, delimiter = ',')\n","np.savetxt('source_test_tensor.csv', source_test_tensor, delimiter = ',')\n","np.savetxt('target_train_tensor.csv', target_train_tensor, delimiter = ',')\n","np.savetxt('target_test_tensor.csv', target_test_tensor, delimiter = ',')\n","\n","\n","\"\"\"\n","For translating mode\n","\"\"\"\n","# # load numpy array from csv file:\n","# load_ar = np.loadtxt('source_train_tensor.csv', delimiter = ',')"]},{"cell_type":"code","execution_count":99,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1660753586587,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"A2NUoadXYGf3"},"outputs":[],"source":["max_source_length= max(len(t) for t in source_tensor)\n","max_target_length= max(len(t) for t in  target_tensor)\n"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1660753586947,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"kPhYmti0YGf3","outputId":"b12e6547-7af6-4c3f-ad17-6ee59707fb9b"},"outputs":[{"data":{"text/plain":["(20, 19)"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["max_source_length, max_target_length"]},{"cell_type":"code","execution_count":101,"metadata":{"executionInfo":{"elapsed":877,"status":"ok","timestamp":1660753588160,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"igd_hwb9iTGU"},"outputs":[],"source":["BATCH_SIZE = 32\n","#Create training dataset and shuffle\n","dataset_train = tf.data.Dataset.from_tensor_slices((source_train_tensor, target_train_tensor)).shuffle(BATCH_SIZE)\n","# divide into batches\n","dataset_train = dataset_train.batch(BATCH_SIZE, drop_remainder=True)\n","\n","#Create test dataset\n","dataset_test = tf.data.Dataset.from_tensor_slices((source_test_tensor, target_test_tensor)).shuffle(BATCH_SIZE)\n","dataset_test = dataset_test.batch(BATCH_SIZE, drop_remainder=True)\n"]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":385,"status":"ok","timestamp":1660753588911,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"vtr_qVkriTGV","outputId":"14da9cc9-dd30-4323-b623-edd9031c361f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 20) (32, 19)\n"]}],"source":["source_batch_train, target_batch_train =next(iter(dataset_train))\n","print(source_batch_train.shape, target_batch_train.shape)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"EhuSjWSkYGf4"},"source":["<h3> Parameters 6 </h3>"]},{"cell_type":"code","execution_count":103,"metadata":{"executionInfo":{"elapsed":242,"status":"ok","timestamp":1660755527254,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"mgZTVQLSYGf4"},"outputs":[],"source":["# Transformer arguments: \n","# num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","# target_vocab_size, max_positional_encoding_input,\n","# max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6\n","\n","num_layers = 4\n","embedding_dim = 128\n","num_heads = 5\n","fully_connected_dim = 512\n","input_vocab_size = num_tokens_source\n","target_vocab_size = num_tokens_target\n","max_positional_encoding_input = max_source_length\n","max_positional_encoding_target = max_target_length"]},{"cell_type":"markdown","metadata":{"id":"Ly4vh0t85N3X"},"source":["<h3> Modify parameter number </h3>"]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":229,"status":"ok","timestamp":1660755530184,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Z4gCP4450_RN"},"outputs":[],"source":["# params_3 = f'\\n params_3: num_layers: {num_layers}, embedding_dim: {embedding_dim}, num_heads: {num_heads}, \\\n","# fully_connected_dim: {fully_connected_dim}, input_vocab_size: {input_vocab_size}, target_vocab_size: {target_vocab_size}, \\\n","# max_positional_encoding_input: {max_positional_encoding_input}, max_positional_encoding_target: {max_positional_encoding_target}'"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1660757266179,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"S8P8Bg8MYGf4","outputId":"77b53229-f388-493a-e52f-674d13b3d53e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 20, 32)\n","(1, 19, 32)\n"]}],"source":["# Transformer arguments: \n","# num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n","# target_vocab_size, max_positional_encoding_input,\n","# max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6\n","\n","transformer = Transformer(\n","    num_layers=num_layers,\n","    embedding_dim=embedding_dim,\n","    num_heads=num_heads,\n","    fully_connected_dim=fully_connected_dim,\n","    input_vocab_size=input_vocab_size,\n","    target_vocab_size=target_vocab_size,\n","    max_positional_encoding_input = max_positional_encoding_input,\n","    max_positional_encoding_target = max_positional_encoding_target\n","    )"]},{"cell_type":"markdown","metadata":{"id":"uvAuEWUwYGf5"},"source":["- Create optimizer\n","- Use customised learning rate as defined in 'Attention Is All You Need' paper\n","- The learning rate increases linearly until \"warmup_steps\" training steps, then decays asymptotically\n","- Inputs: d_model, warmup_steps (default = 4000)"]},{"cell_type":"code","execution_count":106,"metadata":{"executionInfo":{"elapsed":282,"status":"ok","timestamp":1660757268527,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"8qYL_Vg7YGf5"},"outputs":[],"source":["learning_rate = CustomSchedule(embedding_dim, warmup_steps = 2500)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n","                                     epsilon=1e-9)"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":465,"status":"ok","timestamp":1660757270498,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"Ka5uNFNEYGf5"},"outputs":[],"source":["# define loss object\n","# from_logits = False, because we apply softmax to final Dense layer of Transformer\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=False, reduction='none')\n"]},{"cell_type":"code","execution_count":108,"metadata":{"executionInfo":{"elapsed":304,"status":"ok","timestamp":1660757272651,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"P4p4NUnqYGf5"},"outputs":[],"source":["def loss_function(real, pred):\n","                                                            # real = (m, Ty)\n","                                                            # pred = (m, Ty, num_tokens_target)\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))        # want to select only non-zero values\n","                                                            # mask = (m, Ty), and is \"True\" for non-zero values\n","\n","  loss_ = loss_object(real, pred)                           # compute loss for each time-step\n","                                                            # loss = (m, Ty)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)                   \n","  loss_ *= mask                                             # only count loss from non-zero values\n","\n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)           # divide sum(loss) by number of non-zero values\n","\n","def accuracy_function(real, pred):                          # pred = (m, Ty, num_tokens_target)\n","  \n","  accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), tf.int32))      # accuracies = (m, Ty) -- binary values\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))        # mask = (m, Ty) -- boolean values\n","  accuracies = tf.math.logical_and(mask, accuracies)        # suppress values where real value is 0\n","\n","  accuracies = tf.cast(accuracies, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)      # divide sum of 1s in \"accuracies\" by sum of 1s in \"mask\""]},{"cell_type":"code","execution_count":109,"metadata":{"executionInfo":{"elapsed":272,"status":"ok","timestamp":1660757274023,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"objpWrBgYGf5"},"outputs":[],"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n","\n","test_loss = tf.keras.metrics.Mean(name = 'test_loss')\n","test_accuracy = tf.keras.metrics.Mean(name = 'test_accuracy')"]},{"cell_type":"code","execution_count":110,"metadata":{"executionInfo":{"elapsed":368,"status":"ok","timestamp":1660757275756,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"guju7vbIYGf5"},"outputs":[],"source":["@tf.function\n","def train_step(inp, tar):\n","                            # inp = (m, Tx)\n","                            # tar = (m, Ty)\n","\n","\n","  tar_inp = tar[:, :-1]     # \"start_\" to last word\n","  tar_real = tar[:, 1:]     # first word to \"_end\"\n","\n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inputs = (inp, tar_inp),\n","                                 training = True)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  acc = accuracy_function(tar_real, predictions)\n","\n","  # store cumulative loss and acc in train_loss and train_accuracy\n","  train_loss(loss)\n","  train_accuracy(acc)"]},{"cell_type":"code","execution_count":111,"metadata":{},"outputs":[],"source":["def compute_test_loss_acc(inp, tar):\n","    # inp = (m, Tx)\n","    # tar = (m, Ty)\n","\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    predictions, _ = transformer (inputs = (inp, tar_inp), training = False)\n","    test_loss = loss_function(tar_real, predictions)\n","    test_acc = accuracy_function(tar_real, predictions)\n","\n","    return test_loss, test_acc"]},{"cell_type":"code","execution_count":112,"metadata":{"executionInfo":{"elapsed":274,"status":"ok","timestamp":1660757277169,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"-jxQm8iSYGf5"},"outputs":[],"source":["# checkpoint_path = './checkpoints/train'\n","\n","# ckpt = tf.train.Checkpoint(optimizer=optimizer,\n","#                                  transformer=transformer)\n","\n","# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep = 3)\n","# if ckpt_manager.latest_checkpoint:\n","#     ckpt.restore(ckpt_manager.latest_checkpoint)\n","#     print('Latest checkpoint restored!')"]},{"cell_type":"code","execution_count":113,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1558553,"status":"error","timestamp":1660758836991,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"PXsDysBPYGf6","outputId":"a788c160-c44b-43be-931f-360a1b4922f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 Batch 0 -- Train_Loss: 6.4782 Train_Accuracy: 0.0041\n","Summary -- Epoch 1 Train_Loss: 6.4721 Train_Accuracy: 0.0014     Test_Loss: 6.4957 Test_Accuracy: 0.0000\n","Time taken for 1 epoch: 4.64 secs\n","\n","Epoch 2 Batch 0 -- Train_Loss: 6.4691 Train_Accuracy: 0.0043\n","Summary -- Epoch 2 Train_Loss: 6.4643 Train_Accuracy: 0.0021     Test_Loss: 6.4419 Test_Accuracy: 0.0000\n","Time taken for 1 epoch: 0.39 secs\n","\n","Epoch 3 Batch 0 -- Train_Loss: 6.4627 Train_Accuracy: 0.0043\n","Summary -- Epoch 3 Train_Loss: 6.4345 Train_Accuracy: 0.0028     Test_Loss: 6.3969 Test_Accuracy: 0.0000\n","Time taken for 1 epoch: 0.38 secs\n","\n","Epoch 4 Batch 0 -- Train_Loss: 6.3918 Train_Accuracy: 0.0041\n","Summary -- Epoch 4 Train_Loss: 6.3895 Train_Accuracy: 0.0027     Test_Loss: 6.3710 Test_Accuracy: 0.0000\n","Time taken for 1 epoch: 0.38 secs\n","\n","Epoch 5 Batch 0 -- Train_Loss: 6.3708 Train_Accuracy: 0.0086\n","Summary -- Epoch 5 Train_Loss: 6.3427 Train_Accuracy: 0.0108     Test_Loss: 6.3037 Test_Accuracy: 0.0131\n","Time taken for 1 epoch: 0.39 secs\n","\n","Epoch 6 Batch 0 -- Train_Loss: 6.3062 Train_Accuracy: 0.0407\n","Summary -- Epoch 6 Train_Loss: 6.2923 Train_Accuracy: 0.0596     Test_Loss: 6.2772 Test_Accuracy: 0.0304\n","Time taken for 1 epoch: 0.38 secs\n","\n","Epoch 7 Batch 0 -- Train_Loss: 6.2703 Train_Accuracy: 0.0783\n","Summary -- Epoch 7 Train_Loss: 6.2497 Train_Accuracy: 0.1072     Test_Loss: 6.2446 Test_Accuracy: 0.0478\n","Time taken for 1 epoch: 0.38 secs\n","\n","Epoch 8 Batch 0 -- Train_Loss: 6.2282 Train_Accuracy: 0.1299\n","Summary -- Epoch 8 Train_Loss: 6.2077 Train_Accuracy: 0.1352     Test_Loss: 6.1927 Test_Accuracy: 0.0606\n","Time taken for 1 epoch: 0.39 secs\n","\n","Epoch 9 Batch 0 -- Train_Loss: 6.1610 Train_Accuracy: 0.1429\n","Summary -- Epoch 9 Train_Loss: 6.1889 Train_Accuracy: 0.1382     Test_Loss: 6.2223 Test_Accuracy: 0.0707\n","Time taken for 1 epoch: 0.38 secs\n","\n","Epoch 10 Batch 0 -- Train_Loss: 6.1392 Train_Accuracy: 0.1368\n","Summary -- Epoch 10 Train_Loss: 6.1589 Train_Accuracy: 0.1373     Test_Loss: 6.1817 Test_Accuracy: 0.0793\n","Time taken for 1 epoch: 0.38 secs\n","\n"]}],"source":["EPOCHS = 10\n","# train_loss_dict = {}\n","# train_acc_dict = {}\n","# test_loss_dict = {}\n","# test_acc_dict = {}\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_loss_list = []\n","test_acc_list = []\n","\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # reset tf Mean objects\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  test_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  # iterate over every batch (= (inp, tar) tuple) in training dataset\n","  for (batch, (inp, tar)) in enumerate(dataset_train):\n","    train_step(inp, tar)\n","\n","    if batch % 50 == 0:\n","      print(f'Epoch {epoch + 1} Batch {batch} -- Train_Loss: {train_loss.result():.4f} Train_Accuracy: {train_accuracy.result():.4f}')\n","\n","  #if (epoch+1) % 1 == 0:\n","    #ckpt_save_path = ckpt_manager.save()\n","    #print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n","  \n","  # after one epoch of training, compute test loss and test acc\n","  for (batch, (inp, tar)) in enumerate(dataset_test):\n","    test_loss_batch, test_accuracy_batch = compute_test_loss_acc(inp, tar)\n","    # Update tf Mean objects\n","    test_loss(test_loss_batch)\n","    test_accuracy(test_accuracy_batch)\n","  \n","\n","\n","  print(f'Summary -- Epoch {epoch + 1} Train_Loss: {train_loss.result():.4f} Train_Accuracy: {train_accuracy.result():.4f} \\\n","    Test_Loss: {test_loss.result():.4f} Test_Accuracy: {test_accuracy.result():.4f}')\n","  \n","  train_loss_list.append (train_loss.result().numpy())\n","  train_acc_list.append(train_accuracy.result().numpy())\n","\n","  test_loss_list.append(test_loss.result().numpy())\n","  test_acc_list.append(test_accuracy.result().numpy())\n","\n","  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>test_loss</th>\n","      <th>test_acc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>6.472</td>\n","      <td>0.001</td>\n","      <td>6.496</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>6.464</td>\n","      <td>0.002</td>\n","      <td>6.442</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>6.434</td>\n","      <td>0.003</td>\n","      <td>6.397</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>6.390</td>\n","      <td>0.003</td>\n","      <td>6.371</td>\n","      <td>0.000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>6.343</td>\n","      <td>0.011</td>\n","      <td>6.304</td>\n","      <td>0.013</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>6.292</td>\n","      <td>0.060</td>\n","      <td>6.277</td>\n","      <td>0.030</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>6.250</td>\n","      <td>0.107</td>\n","      <td>6.245</td>\n","      <td>0.048</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>6.208</td>\n","      <td>0.135</td>\n","      <td>6.193</td>\n","      <td>0.061</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>6.189</td>\n","      <td>0.138</td>\n","      <td>6.222</td>\n","      <td>0.071</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>6.159</td>\n","      <td>0.137</td>\n","      <td>6.182</td>\n","      <td>0.079</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   epoch  train_loss  train_acc  test_loss  test_acc\n","0      1       6.472      0.001      6.496     0.000\n","1      2       6.464      0.002      6.442     0.000\n","2      3       6.434      0.003      6.397     0.000\n","3      4       6.390      0.003      6.371     0.000\n","4      5       6.343      0.011      6.304     0.013\n","5      6       6.292      0.060      6.277     0.030\n","6      7       6.250      0.107      6.245     0.048\n","7      8       6.208      0.135      6.193     0.061\n","8      9       6.189      0.138      6.222     0.071\n","9     10       6.159      0.137      6.182     0.079"]},"execution_count":116,"metadata":{},"output_type":"execute_result"}],"source":["all_metrics = zip(train_loss_list, train_acc_list, test_loss_list, test_acc_list)\n","df_metrics = pd.DataFrame(all_metrics, columns = ['train_loss', 'train_acc', 'test_loss', 'test_acc']).reset_index().rename(columns = {'index':'epoch'})\n","df_metrics['epoch'] = df_metrics['epoch'].apply(lambda x: x+1)\n","df_metrics = df_metrics.apply(lambda x: round(x, 3))\n","df_metrics"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1660748525271,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"9va6cEFqYGf6"},"outputs":[],"source":["df_metrics.to_csv('df_metrics.csv', index = False)"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1660748525271,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"dTw-6gRR3m1v"},"outputs":[],"source":["# with open(\"params.txt\", \"a\") as text_file:\n","#     text_file.write(params_3)\n","#     #text_file.write('params_3 -- time taken for 1 epoch: 54 secs')"]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":2283,"status":"ok","timestamp":1660748527550,"user":{"displayName":"Kyung","userId":"01072507214168010723"},"user_tz":-120},"id":"C7NiFdQSYGf6"},"outputs":[],"source":["file_path = 'saved_models/model'\n","transformer.save_weights(file_path,save_format='tf')\n","\n","# # Recreate the exact same model purely from the file\n","# new_model = keras.models.load_model('path_to_my_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvmMhr8BOUKd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"train_v1.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 ('deep_learning')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"62f0de500e91648e2f1c8ecd59ca95f97588cc062e27f09a44618e0428f97b74"}}},"nbformat":4,"nbformat_minor":0}
